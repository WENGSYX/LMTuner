{"data": "./data.jsonl", "model": "llama-7b", "method": "LOMO", "learning rate": "1e-5", "epoch": "100", "lora rank": "16", "batch size": "20", "max length": "350", "GPU Number": 2, "GPU Memory": "48GB", "quantization": null, "train this machine": "True", "save interval": "100", "save path": "./checkpoints", "gradient accumulation": "1", "rope scaling": "False", "train continue": false, "load": "", "wandb": "5148a016e44ea8d5a6c00859bcaa6963b690c48d"}